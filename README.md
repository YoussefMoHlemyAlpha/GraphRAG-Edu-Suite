# ğŸ“ GraphRAG Edu-Suite with Quality Control

> Intelligent Educational Content Generation with Comprehensive Quality Assurance

Generate high-quality, context-grounded quiz questions and essay prompts using Graph-based RAG, dual-model AI validation, and comprehensive quality metrics.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/streamlit-1.28+-red.svg)](https://streamlit.io/)
[![Neo4j](https://img.shields.io/badge/neo4j-5.0+-green.svg)](https://neo4j.com/)

---

## ğŸŒŸ Key Features

### ğŸ¯ Quality-Controlled Question Generation
- **Multiple Choice Questions (MCQs)**: 6-10 questions with Bloom's Taxonomy coverage
- **Essay Questions**: 2-5 deep-reasoning prompts with expected concepts
- **Per-Question Metrics**: Individual quality scores for each question
- **Automatic Filtering**: Low-quality questions rejected automatically (>50% coverage threshold)
- **Answer Verification**: Validates correct answers for MCQs
- **Concept Verification**: Checks expected concepts for essays

### ğŸ“Š Comprehensive RAG Metrics
- **Groundedness**: Measures how well content is supported by source material (target: >90%)
- **Hallucination Rate**: Tracks content not found in retrieved context (target: <25%)
- **Overall Quality**: Combined score (target: >80% for "Excellent")
- **Real-time Tracking**: Monitor quality across all generated content

### ğŸ” Advanced Validation System
- **Dual-Model Architecture**: Gemma3:4b for generation, Llama 3.2 for validation
- **Coverage Checks**: Questions must have >50% term overlap with context
- **Critic Validation**: 5-point validation checklist for every question
- **Fuzzy Matching**: Reduces false positives for technical terms

### ğŸ“ˆ Proven Quality Improvements
- **50% reduction** in hallucination rate (50.6% â†’ 20-25%)
- **100% fix** for wrong answers (2/6 â†’ 0/6)
- **11% increase** in overall quality (74.7% â†’ 83-87%)
- **100% question generation** (3/6 â†’ 6/6)

---

## ğŸš€ Quick Start

### Prerequisites

1. **Python 3.8+**
2. **Neo4j Database** (running locally or remotely)
3. **Ollama** with required models

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/GraphRAG-Edu-Suite.git
cd GraphRAG-Edu-Suite

# Install dependencies
pip install -r requirements.txt

# Pull required AI models
ollama pull gemma3:4b
ollama pull llama3.2:latest

# Configure environment
cp .env.example .env
# Edit .env with your Neo4j credentials:
# NEO4J_URI=bolt://localhost:7687
# NEO4J_USERNAME=neo4j
# NEO4J_PASSWORD=your_password
```

### Run the Application

```bash
streamlit run main.py
```

Visit `http://localhost:8501` in your browser!

---

## ğŸ“– How It Works

### 1ï¸âƒ£ Upload Content
Upload PDF lecture materials to build your knowledge graph. The system extracts entities, relationships, and concepts using Gemma3.

### 2ï¸âƒ£ Generate Questions
Choose between MCQs or Essays, select your lesson, and specify the number of questions.

### 3ï¸âƒ£ Automatic Quality Validation
The system automatically:
- Retrieves 1500+ fys, select lesson and question count

### 3. Quality Validation
System automatically:
- Retrieves 1500+ facts from knowledge graph
- Generates questions with Gemma3
- Validates with Llama 3.2 critic
- Filters by coverage thresholds
- Calculates RAG metrics

### 4. Review & Use
See per-question quality metrics and use high-quality content!

---

## ğŸ“Š Example Output

### MCQ with Quality Metrics:
```
Q1: Understand
Which of the following best describes Data-Driven AI?

Groundedness: 100% âœ… Good
Hallucination: 20% âœ… Low

A. An AI approach that learns from examples and data. âœ“
B. A system that relies solely on human-defined rules.
C. A system that mimics human expert decision-making.
D. A system using symbolic reasoning.
```

### Overall Quality:
```
ğŸ“Š Overall Quiz Quality
Average Groundedness: 100%
Average Hallucination: ğŸŸ¢ 24%
Overall Quality: 88% â­ Excellent
```

---

## ğŸ—ï¸ Architecture

```
User Interface (Streamlit)
    â†“
Generation Pipeline
    â”œâ”€ Context Retrieval (Neo4j)
    â”œâ”€ Generation (Gemma3:4b)
    â”œâ”€ Validation (Llama 3.2)
    â”œâ”€ Coverage Filtering
    â””â”€ RAG Metrics
    â†“
Knowledge Graph (Neo4j)
```

---

## ğŸ“ Project Structure

```
GenertaiveQestionsModel/
â”œâ”€â”€ main.py                 # Streamlit UI
â”œâ”€â”€ engine/
â”‚   â”œâ”€â”€ processor.py        # PDF processing
â”‚   â”œâ”€â”€ generator.py        # Question generation
â”‚   â”œâ”€â”€ graph_store.py      # Neo4j operations
â”‚   â”œâ”€â”€ rag_metrics.py      # Metrics calculation
â”‚   â””â”€â”€ vram_util.py        # Model management
â”œâ”€â”€ docs/                   # Documentation
â”œâ”€â”€ .env                    # Configuration
â””â”€â”€ requirements.txt        # Dependencies
```

---

## ğŸ¯ Quality Metrics Explained

### Groundedness
Measures how well questions are supported by source material
- **Formula**: (Supported Sentences) / (Total Sentences)
- **Target**: >90%

### Hallucination Rate
Tracks content not found in retrieved context
- **Formula**: (Words NOT in Context) / (Total Unique Words)
- **Target**: <25%

### Overall Quality
Combined quality score
- **Formula**: (Groundedness Ã— 50%) + ((1 - Hallucination) Ã— 50%)
- **Target**: >80% for "Excellent"

---

## ğŸ“š Documentation

- [Project Overview](docs/PROJECT_OVERVIEW_V2.md) - Complete system documentation
- [Metrics Guide](docs/METRICS_CALCULATION_EXPLAINED.md) - How metrics are calculated
- [Visual Guide](docs/METRICS_VISUAL_GUIDE.md) - Visual explanations
- [Testing Guide](docs/TESTING_IMPROVEMENTS.md) - How to test improvements
- [Before/After](docs/BEFORE_AFTER_COMPARISON.md) - Quality improvements comparison

---

## ğŸ”§ Configuration

### Environment Variables (.env)
```bash
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your_password
```

### Model Configuration
- **Gemma3:4b**: Generation model (questions, essays)
- **Llama 3.2**: Validation model (critic, grading)

### Quality Thresholds
- Question coverage: >50% (configurable in `engine/generator.py`)
- Answer coverage: >40% (configurable in `engine/generator.py`)
- Concept coverage: >40% (configurable in `engine/generator.py`)

---

## ğŸ“ˆ Performance

### Before Quality Control:
- Hallucination: 50.6%
- Wrong answers: 2/6 questions
- Question count: 3/6 generated

### After Quality Control:
- Hallucination: 20-25% âœ…
- Wrong answers: 0/6 questions âœ…
- Question count: 6/6 generated âœ…

---

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:
- Additional LLM support
- Enhanced metrics algorithms
- UI/UX improvements
- Documentation
- Bug fixes


---

## ğŸ™ Acknowledgments

- Ollama for local LLM inference
- Neo4j for graph database
- Streamlit for web framework
- LangChain for LLM orchestration


---

## ğŸ“ Use Cases

### For Educators:
- Generate quiz questions from lectures
- Create essay prompts with quality assurance
- Track content quality
- Ensure questions are answerable

### For Students:
- Practice with high-quality questions
- Get clear essay expectations
- Receive AI-powered feedback
- Study with Bloom's Taxonomy alignment

### For Researchers:
- Experiment with RAG metrics
- Study hallucination reduction
- Analyze knowledge graphs
- Benchmark LLM performance

---

**Version**: 2.0 (Quality Control Update)  
**Status**: Production Ready âœ…  
**Last Updated**: February 2026

---

Made with â¤ï¸ using Gemma3, Llama 3.2, Neo4j, and Streamlit
